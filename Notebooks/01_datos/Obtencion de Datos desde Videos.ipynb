{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ec7aa3",
   "metadata": {},
   "source": [
    "# INSTALACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33477c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83c24a",
   "metadata": {},
   "source": [
    "# IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fda5de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb425e",
   "metadata": {},
   "source": [
    "# Funcionalidades para MediaPipe\n",
    "\n",
    "- Import de modelo\n",
    "- mediapipe_detection: Deteccion de Puntos Corporales por frame/imagen\n",
    "- draw_styled_landmarks: dibuja puntos corporales sobre la imagen/frame recibido\n",
    "- array_from_landmarks: retorna un arreglo concatenando puntos de [cara, pose, mano izquierda, mano derecha] para los resultados de 1 frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237cf0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mediapipe.solutions.holistic\n",
    "mp_drawing = mediapipe.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad89a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)                 #prediction from a frame\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c2e169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "                # [color1 (B), color2 (G), color3 (R), thickness, circleRadius]\n",
    "leftHandStyles =  [0, 138, 255, 2, 1] #naranja\n",
    "rightHandStyles = [231, 217, 0, 2, 1] #celeste\n",
    "faceStyles =      [80, 110, 10, 0, 1]\n",
    "poseStyles =      [70, 100, 5, 2, 1]\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    #     FACE\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        mp_drawing.DrawingSpec(color=(faceStyles[0],faceStyles[1],faceStyles[2]), thickness=faceStyles[3], circle_radius=faceStyles[4])\n",
    "    )\n",
    "    #     POSE/BODY\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(poseStyles[0],poseStyles[1],poseStyles[2]), thickness=poseStyles[3], circle_radius=poseStyles[4])\n",
    "    )\n",
    "    #     LEFT HAND\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.left_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(leftHandStyles[0],leftHandStyles[1],leftHandStyles[2]), thickness=leftHandStyles[3], circle_radius=leftHandStyles[4])\n",
    "    )\n",
    "    #     RIGHT HAND\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.right_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(rightHandStyles[0],rightHandStyles[1],rightHandStyles[2]), thickness=rightHandStyles[3], circle_radius=rightHandStyles[4])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE SAME FROM ABOVE BUT WITHOUT z and visibility\n",
    "def array_from_landmarks(results):\n",
    "    # LEFT HAND array\n",
    "    # len(results.left_hand_landmarks.landmark) = 21 landmarks for each hand, with 2 coordinates each landmark\n",
    "    if (results.left_hand_landmarks):\n",
    "        leftHandLandmarks = np.array([[result.x, result.y] for result in results.left_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        leftHandLandmarks = np.zeros(21*2)\n",
    "\n",
    "\n",
    "    # RIGHT HAND array\n",
    "    # len(results.right_hand_landmarks.landmark) -> same for right hand\n",
    "    if (results.right_hand_landmarks):\n",
    "        rightHandLandmarks = np.array([[result.x, result.y ] for result in results.right_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        rightHandLandmarks = np.zeros(21*2)\n",
    "\n",
    "\n",
    "    # POSE array\n",
    "    # len(results.pose_landmarks.landmark) -> 33 landmarks of 2 coordinates each one (X, Y)\n",
    "    if (results.pose_landmarks):\n",
    "        poseLandmarks = np.array([[result.x, result.y] for result in results.pose_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        poseLandmarks = np.zeros(33*2)\n",
    "\n",
    "\n",
    "    # FACE array\n",
    "    # len(results.face_landmarks.landmark) -> 468 landmarks of 2 coordinates each one\n",
    "    if (results.face_landmarks):\n",
    "        faceLandmarks = np.array([[result.x, result.y] for result in results.face_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        faceLandmarks = np.zeros(468*2)\n",
    "        \n",
    "    return np.concatenate((faceLandmarks, poseLandmarks, leftHandLandmarks, rightHandLandmarks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00e358",
   "metadata": {},
   "source": [
    "# TRATADO DE VIDEOS\n",
    "\n",
    "- Celda que retorna los valores maximos y minimos que tienen los videos de una se침a.\n",
    "- Definicion de: Path/Ruta de alamcenamiento, Lista de Palabras, Cantidad de Personas, Cantidad de Videos por Persona y numero maximo de Frames (hallado con la celda superior).\n",
    "- Creacion de carpetas para almacenar los resultados de cada video\n",
    "- Funcion que realiza Padding a derecha a cada video que no alcanza la cantidad de frames \"maximumNumberOfFrames\", esto hace que todos los arreglos generados tengan la misma longitud.\n",
    "- Procesado de videos, se procesa cada frame de cada video con MediaPipe y se almacena el resultado de cada video como un arreglo de frames, donde cada frame tiene la concatenacion de los puntos corporales que entrega array_from_landmarks(). en un archivo de extension \".npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a7842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = np.zeros(64)\n",
    "min_frames = [400 for i in range(64)]\n",
    "for i in range(64):\n",
    "    for j in range(10):\n",
    "        for k in range(5):\n",
    "            cap = cv2.VideoCapture(f'''LSA64/all_cut/0{str(i+1).zfill(2)}_0{str(j+1).zfill(2)}_00{k+1}.mp4''')\n",
    "            length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            if length > max_frames[i]:\n",
    "                max_frames[i] = length\n",
    "            if length < min_frames[i]:\n",
    "                min_frames[i] = length\n",
    "print(max_frames)\n",
    "print(min_frames)\n",
    "i = np.argmax(max_frames)\n",
    "j = np.argmin(min_frames)\n",
    "print(f'''Maximo numero de frames {max_frames[i]}. Se침a: {i}''')\n",
    "print(f'''Minimo numero de frames {min_frames[j]}. Se침a: {j}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53730180",
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE_PATH2 = os.path.join('LSA64_data')\n",
    "\n",
    "# Lista de Palabras con las que se armar치n las oraciones, no se encuentran las 64 palabras\n",
    "signs_list = ['nacer','comida','brillante', 'mujer', 'hijo', 'hombre', 'lejos', 'aprender', 'espumadera','amargo','leche','Uruguay','pais','donde','ninguno','nombre','perfume','sordo','comprar','encontrar', 'nave espacial']\n",
    "numberOfPersons = 10\n",
    "numberOfVideosPerPerson = 5\n",
    "\n",
    "maximumNumberOfFrames = 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385cf331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREACION DE CARPETAS\n",
    "for sign in signs_list:\n",
    "    try:\n",
    "        os.makedirs(os.path.join(STORE_PATH2, sign))\n",
    "        print(f'''Make dir. {sign} ready''')\n",
    "    except FileExistsError:\n",
    "        print(f'''Error: {FileExistsError}, {sign}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valor de padding '3' ya que los valores del arreglo estan normalizados entre [0.0, 1.0]\n",
    "paddingValue = 3\n",
    "def paddData(sequence):\n",
    "    paddLength = maximumNumberOfFrames - len(sequence)\n",
    "    return np.pad(sequence, [(0,paddLength),(0,0)], mode='constant', constant_values=paddingValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b912bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holisticModel:\n",
    "    for sign in range(49,len(signs_list)):\n",
    "        for personNumber in range(numberOfPersons):\n",
    "            for videoNumber in range(numberOfVideosPerPerson):\n",
    "                caption = cv2.VideoCapture(f'''all_cut/0{str(sign+1).zfill(2)}_0{str(personNumber+1).zfill(2)}_00{videoNumber+1}.mp4''')\n",
    "                print(f'''Leo: 0{str(sign+1).zfill(2)}_0{str(personNumber+1).zfill(2)}_00{videoNumber+1}.mp4''')\n",
    "                if (caption.isOpened() == False):\n",
    "                    print(\"Error opening video stream or file\")\n",
    "                    break\n",
    "                keypointsSequence = []\n",
    "                while(caption.isOpened()):\n",
    "                    ret, frame = caption.read()\n",
    "                    if (ret):\n",
    "                        image, results = mediapipe_detection(frame, holisticModel)\n",
    "#                         draw_styled_landmarks(image, results)\n",
    "#                         cv2.imshow('Frame',image) \n",
    "                        keypoints = array_from_landmarks(results)\n",
    "                        keypointsSequence.append(keypoints)\n",
    "                    else:\n",
    "                        break\n",
    "                    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                        # Break the loop\n",
    "                        break\n",
    "                a = keypointsSequence\n",
    "                if len(keypointsSequence) < maximumNumberOfFrames:\n",
    "                    a = paddData(keypointsSequence)\n",
    "                localPath = os.path.join(STORE_PATH2, signs_list[sign], f'''0{str(personNumber)}_0{str(videoNumber)}''')\n",
    "                np.save(localPath, a)\n",
    "                print(f'''Guardo en: {STORE_PATH2}/{signs_list[sign]}/0{str(personNumber)}_0{str(videoNumber)}''')\n",
    "# release the video capture object\n",
    "caption.release()\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
