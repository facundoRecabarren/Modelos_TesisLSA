{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import mediapipe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARGA MODELOS\n",
    "\n",
    "Colocar el nombre/ruta del modelo a probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    }
   ],
   "source": [
    "mp_holistic = mediapipe.solutions.holistic\n",
    "mp_drawing = mediapipe.solutions.drawing_utils\n",
    "\n",
    "model = XGBClassifier(eval_metric='mlogloss')\n",
    "\n",
    "# Colocar el nombre del modelo a probar\n",
    "model.load_model('../../02_modelos/XGBoostMuestreoRandom')\n",
    "\n",
    "print(model.classes_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILIDADES\n",
    "Funciones para obtencion de puntos, dibujar puntos sobre imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNS_LIST = ['nacer','comida','brillante', 'mujer', 'hijo', 'hombre', 'lejos', 'aprender', 'espumadera','amargo','leche','Uruguay','pais','donde','ninguno','nombre','perfume','sordo','comprar','encontrar', 'nave espacial']\n",
    "# signs_list = ['nacer']\n",
    "\n",
    "# se toma la lista original de se침as ya que estas en el orden en que se encuentran, su indice representa correctamente el nombre del video original\n",
    "FULL_SIGNS_LIST = ['opaco', 'rojo', 'verde', 'amarillo', 'brillante', 'celeste', 'colores', 'rosa', 'mujer', 'enemigo', 'hijo', 'hombre', 'lejos','caj칩n','nacer','aprender','llamar','espumadera','amargo','dulce','leche','agua','comida','Argentina','Uruguay','pais','donde','apellido','burla','cumpleanos','desayuno','foto','hambre','mapa','moneda','musica','nave espacial','ninguno','nombre','paciencia','perfume','sordo','trampa','arroz','asado','caramelo','chicle','fideos','yogurt','aceptar','agradecer','apagar','aparecer','aterrizar','atrapar','ayudar','bailar','ba침arse','comprar','copiar','correr','darse cuenta','dar','encontrar']\n",
    "\n",
    "NUMBER_OF_PERSONS = 10\n",
    "NUMBER_OF_VIDEOS_PER_PERSON = 5\n",
    "\n",
    "frame_columns = []\n",
    "\n",
    "AMOUNT_OF_FRAMES = 10\n",
    "# amount_of_frames = 30\n",
    "\n",
    "FACE_POINTS = 468\n",
    "posePointIndexes = [k for k in range(23)] #de los 33 puntos solo tomamos los hombros, brazos, cabeza (SIN CINTURA debido a los videos del set de datos)\n",
    "LEFT_HAND_POINTS = 21\n",
    "RIGHT_HAND_POINTS = 21\n",
    "\n",
    "# CAMBIAR ESTE BOOLEANO PARA TOMAR PUNTOS FACIALES O NO (se tomar치n CEJAS y BOCA)\n",
    "USE_FACE_POINTS = False\n",
    "\n",
    "# https://github.com/tensorflow/tfjs-models/commit/838611c02f51159afdd77469ce67f0e26b7bbb23#diff-e5d31503f11c6bae62542ea89982152514b81906dff0b718e44708bcf22aa361\n",
    "# https://github.com/ManuelTS/augmentedFaceMeshIndices/blob/master/Left_Eye.jpg\n",
    "# https://github.com/google/mediapipe/blob/a908d668c730da128dfa8d9f6bd25d519d006692/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\n",
    "\n",
    "# No he repetido los puntos que tenian en comun ciertos arreglos, como los puntos faciales en los datos\n",
    "# obtenidos de los videos se encuentran primeros, estos indices se corresponden\n",
    "\n",
    "rightEyebrowUpper = [156, 70, 63, 105, 66, 107, 55, 193]\n",
    "rightEyebrowLower = [35, 124, 46, 53, 52, 65]\n",
    "\n",
    "leftEyebrowUpper  = [383, 300, 293, 334, 296, 336, 285, 417]\n",
    "leftEyebrowLower  = [265, 353, 276, 283, 282, 295]\n",
    "    \n",
    "lipsUpperOuter    = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]\n",
    "lipsLowerOuter    = [146, 91, 181, 84, 17, 314, 405, 321, 375]\n",
    "lipsUpperInner    = [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308]\n",
    "lipsLowerInner    = [95, 88, 178, 87, 14, 317, 402, 318, 324]\n",
    "\n",
    "facePointsIndexes = rightEyebrowUpper + rightEyebrowLower + leftEyebrowUpper + leftEyebrowLower + lipsUpperOuter + lipsLowerOuter + lipsUpperInner + lipsLowerInner\n",
    "\n",
    "for frame in range(AMOUNT_OF_FRAMES):\n",
    "    \n",
    "    if (USE_FACE_POINTS):\n",
    "        for index in range (len(facePointsIndexes)):\n",
    "            frame_columns.append(f'''fr_{frame}_face_p{index}_x''')\n",
    "            frame_columns.append(f'''fr_{frame}_face_p{index}_y''')\n",
    "    \n",
    "    for index in range (len(posePointIndexes)):\n",
    "        frame_columns.append(f'''fr_{frame}_pose_p{index}_x''')\n",
    "        frame_columns.append(f'''fr_{frame}_pose_p{index}_y''')\n",
    "\n",
    "    for index in range (LEFT_HAND_POINTS):\n",
    "        frame_columns.append(f'''fr_{frame}_left_hand_p{index}_x''')\n",
    "        frame_columns.append(f'''fr_{frame}_left_hand_p{index}_y''')\n",
    "\n",
    "    for index in range (RIGHT_HAND_POINTS):\n",
    "        frame_columns.append(f'''fr_{frame}_right_hand_p{index}_x''')\n",
    "        frame_columns.append(f'''fr_{frame}_right_hand_p{index}_y''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)                 #prediction from a frame\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "# COLORES PARA PUNTOS Y LINEAS A DIBUJAR EN LA IMAGEN\n",
    "leftHandStyles = [0,138,255,2,1] #naranja\n",
    "rightHandStyles = [231,217,0,2,1] #celeste\n",
    "faceStyles = [80,110,10,0,1]\n",
    "poseStyles = [70,100,5,2,1]\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    #     FACE\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        mp_drawing.DrawingSpec(color=(faceStyles[0],faceStyles[1],faceStyles[2]), thickness=faceStyles[3], circle_radius=faceStyles[4])\n",
    "    )\n",
    "    #     POSE/BODY\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(poseStyles[0],poseStyles[1],poseStyles[2]), thickness=poseStyles[3], circle_radius=poseStyles[4])\n",
    "    )\n",
    "    #     LEFT HAND\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.left_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(leftHandStyles[0],leftHandStyles[1],leftHandStyles[2]), thickness=leftHandStyles[3], circle_radius=leftHandStyles[4])\n",
    "    )\n",
    "    #     RIGHT HAND\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.right_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(rightHandStyles[0],rightHandStyles[1],rightHandStyles[2]), thickness=rightHandStyles[3], circle_radius=rightHandStyles[4])\n",
    "    )\n",
    "\n",
    "def array_from_landmarks(results):\n",
    "    # LEFT HAND array\n",
    "    # len(results.left_hand_landmarks.landmark) = 21 landmarks for each hand, with 2 coordinates each landmark\n",
    "    if (results.left_hand_landmarks):\n",
    "        leftHandLandmarks = np.array([[result.x, result.y] for result in results.left_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        leftHandLandmarks = np.zeros(21*2)\n",
    "\n",
    "\n",
    "    # RIGHT HAND array\n",
    "    # len(results.right_hand_landmarks.landmark) -> same for right hand\n",
    "    if (results.right_hand_landmarks):\n",
    "        rightHandLandmarks = np.array([[result.x, result.y ] for result in results.right_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        rightHandLandmarks = np.zeros(21*2)\n",
    "\n",
    "\n",
    "    # POSE array\n",
    "    # len(results.pose_landmarks.landmark) -> 33 landmarks of 2 coordinates each one (X, Y)\n",
    "    if (results.pose_landmarks):\n",
    "        poseLandmarks = np.array([[result.x, result.y] for result in results.pose_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        poseLandmarks = np.zeros(33*2)\n",
    "\n",
    "\n",
    "    # FACE array\n",
    "    # len(results.face_landmarks.landmark) -> 468 landmarks of 2 coordinates each one\n",
    "    if (results.face_landmarks):\n",
    "        faceLandmarks = np.array([[result.x, result.y] for result in results.face_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        faceLandmarks = np.zeros(468*2)\n",
    "        \n",
    "    frame = np.concatenate((faceLandmarks, poseLandmarks, leftHandLandmarks, rightHandLandmarks))\n",
    "    \n",
    "    values_validation(frame)\n",
    "    \n",
    "    aux_array = []\n",
    "    #CARA\n",
    "    if (USE_FACE_POINTS):\n",
    "        for faceKeypoint in facePointsIndexes:\n",
    "            aux_array.append(frame[faceKeypoint*2])     # X\n",
    "            aux_array.append(frame[faceKeypoint*2+1])   # Y\n",
    "    #POSE\n",
    "    for poseKeypoint in range(936, 936+len(posePointIndexes)*2):\n",
    "        aux_array.append(frame[poseKeypoint])\n",
    "    #MANOS\n",
    "    for keypoint in range(1002, 1086):\n",
    "        aux_array.append(frame[keypoint])\n",
    "                \n",
    "    return aux_array\n",
    "\n",
    "def values_validation(frame):\n",
    "    for i in range(len(frame)):\n",
    "        if (frame[i] > 1):\n",
    "            frame[i] = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRUEBA CON CAMARA EN TIEMPO REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = cv2.VideoCapture(0)\n",
    "caption.set(cv2.CAP_PROP_FPS, 60)\n",
    "resultsForVideo = []\n",
    "\n",
    "framePointsLength = int(len(frame_columns)/AMOUNT_OF_FRAMES)\n",
    "sentence = \"\"\n",
    "i = 0\n",
    "with mp_holistic.Holistic(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as holisticModel:\n",
    "    framesWindow = []\n",
    "    while caption.isOpened():\n",
    "        ret, frame = caption.read()\n",
    "        i = i + 1\n",
    "        image, results = mediapipe_detection(frame, holisticModel)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = array_from_landmarks(results)\n",
    "        \n",
    "        # SIN SALTO DE FRAMES\n",
    "        framesWindow.extend(keypoints)\n",
    "\n",
    "        #HACE FUNCION DE VENTANA DESLIZANTE DE FRAMES\n",
    "        if (len(framesWindow) > len(frame_columns)):\n",
    "            framesWindow = framesWindow[framePointsLength:]\n",
    "\n",
    "        # PREDICE AL LLEGAR A 10 FRAMES\n",
    "        if (len(framesWindow) >= len(frame_columns)):\n",
    "            #if (lastFrameAdded == i - 1):\n",
    "                #  res = model.predict(np.expand_dims(keypoints, axis=0))\n",
    "            res = model.predict([framesWindow])\n",
    "            resultsForVideo.append(res[0])\n",
    "            \n",
    "            sentence = SIGNS_LIST[res[0]]\n",
    "        \n",
    "        cv2.putText(image, ' '.join(sentence), (10,20), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,255,255), 2)\n",
    "        cv2.putText(image, 'Frame 춿: ' + str(i), (10,50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,255,255), 2)\n",
    "        cv2.imshow('Predicting sign..', image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'): #press 'q' to break\n",
    "            break\n",
    "            \n",
    "    caption.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRUEBA DESDE VIDEOS\n",
    "\n",
    "Para la lista de se침as se procesar치n los videos, de ser necesario, se puede comentar los 3 bucles (for) y colocar el nombre/ruta del video especifico a procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/LSA64/all_cut/005_001_001.mp4\n",
      "C:/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/LSA64/all_cut/005_001_002.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\facur\\Desktop\\tesis_LSA\\codigos_datos_tesis\\Notebooks\\Lengua_Sen팪a_CLF\\03_implementacion\\01_cam_predictor.ipynb Cell 10\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/Notebooks/Lengua_Sen%CC%83a_CLF/03_implementacion/01_cam_predictor.ipynb#X12sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# PREDICE AL LLEGAR A 10 FRAMES\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/Notebooks/Lengua_Sen%CC%83a_CLF/03_implementacion/01_cam_predictor.ipynb#X12sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mlen\u001b[39m(framesWindow) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(frame_columns)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/Notebooks/Lengua_Sen%CC%83a_CLF/03_implementacion/01_cam_predictor.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# if (lastFrameAdded == i - 1):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/Notebooks/Lengua_Sen%CC%83a_CLF/03_implementacion/01_cam_predictor.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m#  res = model.predict(np.expand_dims(keypoints, axis=0))\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/Notebooks/Lengua_Sen%CC%83a_CLF/03_implementacion/01_cam_predictor.ipynb#X12sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     res \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict([framesWindow])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/Notebooks/Lengua_Sen%CC%83a_CLF/03_implementacion/01_cam_predictor.ipynb#X12sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     resultsForVideo\u001b[39m.\u001b[39mappend(res[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/Notebooks/Lengua_Sen%CC%83a_CLF/03_implementacion/01_cam_predictor.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m# resultsIndexes.append(res[0])\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\facur\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\sklearn.py:1525\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m   1516\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1517\u001b[0m     X: ArrayLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     iteration_range: Optional[Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1523\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m   1524\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(verbosity\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbosity):\n\u001b[1;32m-> 1525\u001b[0m         class_probs \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(\n\u001b[0;32m   1526\u001b[0m             X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   1527\u001b[0m             output_margin\u001b[39m=\u001b[39;49moutput_margin,\n\u001b[0;32m   1528\u001b[0m             ntree_limit\u001b[39m=\u001b[39;49mntree_limit,\n\u001b[0;32m   1529\u001b[0m             validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[0;32m   1530\u001b[0m             base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m   1531\u001b[0m             iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[0;32m   1532\u001b[0m         )\n\u001b[0;32m   1533\u001b[0m         \u001b[39mif\u001b[39;00m output_margin:\n\u001b[0;32m   1534\u001b[0m             \u001b[39m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[0;32m   1535\u001b[0m             \u001b[39mreturn\u001b[39;00m class_probs\n",
      "File \u001b[1;32mc:\\Users\\facur\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\sklearn.py:1131\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1127\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m         \u001b[39m# coo, csc, dt\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m-> 1131\u001b[0m test \u001b[39m=\u001b[39m DMatrix(\n\u001b[0;32m   1132\u001b[0m     X,\n\u001b[0;32m   1133\u001b[0m     base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m   1134\u001b[0m     missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[0;32m   1135\u001b[0m     nthread\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m   1136\u001b[0m     feature_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_types,\n\u001b[0;32m   1137\u001b[0m     enable_categorical\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menable_categorical,\n\u001b[0;32m   1138\u001b[0m )\n\u001b[0;32m   1139\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_booster()\u001b[39m.\u001b[39mpredict(\n\u001b[0;32m   1140\u001b[0m     data\u001b[39m=\u001b[39mtest,\n\u001b[0;32m   1141\u001b[0m     iteration_range\u001b[39m=\u001b[39miteration_range,\n\u001b[0;32m   1142\u001b[0m     output_margin\u001b[39m=\u001b[39moutput_margin,\n\u001b[0;32m   1143\u001b[0m     validate_features\u001b[39m=\u001b[39mvalidate_features,\n\u001b[0;32m   1144\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\facur\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\facur\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\core.py:743\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    741\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 743\u001b[0m handle, feature_names, feature_types \u001b[39m=\u001b[39m dispatch_data_backend(\n\u001b[0;32m    744\u001b[0m     data,\n\u001b[0;32m    745\u001b[0m     missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[0;32m    746\u001b[0m     threads\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnthread,\n\u001b[0;32m    747\u001b[0m     feature_names\u001b[39m=\u001b[39;49mfeature_names,\n\u001b[0;32m    748\u001b[0m     feature_types\u001b[39m=\u001b[39;49mfeature_types,\n\u001b[0;32m    749\u001b[0m     enable_categorical\u001b[39m=\u001b[39;49menable_categorical,\n\u001b[0;32m    750\u001b[0m )\n\u001b[0;32m    751\u001b[0m \u001b[39massert\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m handle\n",
      "File \u001b[1;32mc:\\Users\\facur\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\data.py:966\u001b[0m, in \u001b[0;36mdispatch_data_backend\u001b[1;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_uri(data, missing, feature_names, feature_types)\n\u001b[0;32m    965\u001b[0m \u001b[39mif\u001b[39;00m _is_list(data):\n\u001b[1;32m--> 966\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_list(data, missing, threads, feature_names, feature_types)\n\u001b[0;32m    967\u001b[0m \u001b[39mif\u001b[39;00m _is_tuple(data):\n\u001b[0;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_tuple(data, missing, threads, feature_names, feature_types)\n",
      "File \u001b[1;32mc:\\Users\\facur\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\data.py:899\u001b[0m, in \u001b[0;36m_from_list\u001b[1;34m(data, missing, n_threads, feature_names, feature_types)\u001b[0m\n\u001b[0;32m    897\u001b[0m array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(data)\n\u001b[0;32m    898\u001b[0m _check_data_shape(data)\n\u001b[1;32m--> 899\u001b[0m \u001b[39mreturn\u001b[39;00m _from_numpy_array(array, missing, n_threads, feature_names, feature_types)\n",
      "File \u001b[1;32mc:\\Users\\facur\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\data.py:214\u001b[0m, in \u001b[0;36m_from_numpy_array\u001b[1;34m(data, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[0;32m    208\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[0;32m    209\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmissing\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(missing),\n\u001b[0;32m    210\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnthread\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mint\u001b[39m(nthread),\n\u001b[0;32m    211\u001b[0m }\n\u001b[0;32m    212\u001b[0m config \u001b[39m=\u001b[39m \u001b[39mbytes\u001b[39m(json\u001b[39m.\u001b[39mdumps(args), \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    213\u001b[0m _check_call(\n\u001b[1;32m--> 214\u001b[0m     _LIB\u001b[39m.\u001b[39;49mXGDMatrixCreateFromDense(\n\u001b[0;32m    215\u001b[0m         _array_interface(data),\n\u001b[0;32m    216\u001b[0m         config,\n\u001b[0;32m    217\u001b[0m         ctypes\u001b[39m.\u001b[39;49mbyref(handle),\n\u001b[0;32m    218\u001b[0m     )\n\u001b[0;32m    219\u001b[0m )\n\u001b[0;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m handle, feature_names, feature_types\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resultsIndexes = []\n",
    "indexAndName = []\n",
    "framePointsLength = int(len(frame_columns)/AMOUNT_OF_FRAMES)\n",
    "lastFrameAdded = 0\n",
    "with mp_holistic.Holistic(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as holisticModel:\n",
    "    for sign in range(len(FULL_SIGNS_LIST)):\n",
    "        if (SIGNS_LIST.count(FULL_SIGNS_LIST[sign])):\n",
    "            for j in range(NUMBER_OF_PERSONS):\n",
    "                for k in range(NUMBER_OF_VIDEOS_PER_PERSON):\n",
    "                    caption = cv2.VideoCapture(f'''C:/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/LSA64/all_cut/0{str(sign+1).zfill(2)}_0{str(j+1).zfill(2)}_00{k+1}.mp4''')\n",
    "                    print(f'''C:/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/LSA64/all_cut/0{str(sign+1).zfill(2)}_0{str(j+1).zfill(2)}_00{k+1}.mp4''')\n",
    "                    framesLength = int(caption.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                    i = 0\n",
    "                    \n",
    "                    framesWindow = []\n",
    "                    resultsForVideo = []\n",
    "                    while (caption.isOpened()):\n",
    "                        ret, frame = caption.read()\n",
    "                        if (ret):\n",
    "                            image, results = mediapipe_detection(frame, holisticModel)\n",
    "                            draw_styled_landmarks(image, results)\n",
    "                            keypoints = array_from_landmarks(results)\n",
    "                            \n",
    "                            # SE PUEDE PROBAR SALTEANDO/SKIPPEANDO FRAMES DEL VIDEO (== pares) (!= impares)\n",
    "                            # if (i % 2 == 0):\n",
    "                                # framesWindow.extend(keypoints)\n",
    "                                # lastFrameAdded = i\n",
    "                                # i = i + 1\n",
    "                            \n",
    "                            # SIN SALTO DE FRAMES\n",
    "                            framesWindow.extend(keypoints)\n",
    "                            \n",
    "                            #HACE FUNCION DE VENTANA DESLIZANTE DE FRAMES\n",
    "                            if (len(framesWindow) > len(frame_columns)):\n",
    "                                framesWindow = framesWindow[framePointsLength:]\n",
    "                            \n",
    "                            # PREDICE AL LLEGAR A 10 FRAMES\n",
    "                            if (len(framesWindow) >= len(frame_columns)):\n",
    "                                # if (lastFrameAdded == i - 1):\n",
    "                                #  res = model.predict(np.expand_dims(keypoints, axis=0))\n",
    "                                res = model.predict([framesWindow])\n",
    "                                resultsForVideo.append(res[0])\n",
    "                                # resultsIndexes.append(res[0])\n",
    "                            cv2.imshow('Predicting sign..', image)\n",
    "                        else:\n",
    "                            print(\"Se acab칩\")\n",
    "                            resultsIndexes.append(resultsForVideo)\n",
    "                            indexAndName.append({'name': FULL_SIGNS_LIST[sign], 'index': SIGNS_LIST.index(FULL_SIGNS_LIST[sign])})\n",
    "                            break\n",
    "                        if cv2.waitKey(10) & 0xFF == ord('q'): #press 'q' to break\n",
    "                            break\n",
    "                    caption.release()\n",
    "                    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 15, 15, 18, 18]\n",
      "2\n",
      "51::56\n",
      "{'2': 51, '6': 1, '15': 2, '18': 2}\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 6, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]\n",
      "2\n",
      "41::62\n",
      "{'2': 41, '6': 4, '15': 17}\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 15, 15]\n",
      "2\n",
      "55::68\n",
      "{'1': 9, '2': 55, '6': 2, '15': 2}\n"
     ]
    }
   ],
   "source": [
    "for results in range(len(resultsIndexes)):\n",
    "    resultsIndexes[results].sort()\n",
    "    dicts = {}\n",
    "    for element in resultsIndexes[results]:\n",
    "        element = str(element)\n",
    "        if(element in dicts):\n",
    "            dicts[element] = dicts[element] + 1\n",
    "        else:\n",
    "            dicts[element] = 1\n",
    "    print(resultsIndexes[results]) # Muestra el arreglo de predicciones realizadas para el video ordenadas por clase ascendentemente\n",
    "    print(indexAndName[results]['index']) #Muestra el indice de la palabra en el arreglo SIGNS_LIST\n",
    "    # Muestra-compara la cantidad de predicciones correctas contra el total de predicciones realizadas\n",
    "    print(str(resultsIndexes[results].count(indexAndName[results]['index']))+'::'+str(len(resultsIndexes[results])))\n",
    "    print(dicts) # Muestra diccionario de clases predichas y su cantidad para el video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
