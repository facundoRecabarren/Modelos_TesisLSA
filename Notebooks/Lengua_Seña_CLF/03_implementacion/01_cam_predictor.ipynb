{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import mediapipe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARGA MODELOS\n",
    "\n",
    "Colocar el nombre/ruta del modelo a probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    }
   ],
   "source": [
    "mp_holistic = mediapipe.solutions.holistic\n",
    "mp_drawing = mediapipe.solutions.drawing_utils\n",
    "\n",
    "model = XGBClassifier(eval_metric='mlogloss')\n",
    "\n",
    "# Colocar el nombre del modelo a probar\n",
    "model.load_model('../../02_modelos/XGBoostMuestreoRandom')\n",
    "\n",
    "print(model.classes_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILIDADES\n",
    "Funciones para obtencion de puntos, dibujar puntos sobre imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNS_LIST = ['nacer','comida','brillante', 'mujer', 'hijo', 'hombre', 'lejos', 'aprender', 'espumadera','amargo','leche','Uruguay','pais','donde','ninguno','nombre','perfume','sordo','comprar','encontrar', 'nave espacial']\n",
    "# signs_list = ['nacer']\n",
    "\n",
    "# se toma la lista original de se침as ya que estas en el orden en que se encuentran, su indice representa correctamente el nombre del video original\n",
    "FULL_SIGNS_LIST = ['opaco', 'rojo', 'verde', 'amarillo', 'brillante', 'celeste', 'colores', 'rosa', 'mujer', 'enemigo', 'hijo', 'hombre', 'lejos','caj칩n','nacer','aprender','llamar','espumadera','amargo','dulce','leche','agua','comida','Argentina','Uruguay','pais','donde','apellido','burla','cumpleanos','desayuno','foto','hambre','mapa','moneda','musica','nave espacial','ninguno','nombre','paciencia','perfume','sordo','trampa','arroz','asado','caramelo','chicle','fideos','yogurt','aceptar','agradecer','apagar','aparecer','aterrizar','atrapar','ayudar','bailar','ba침arse','comprar','copiar','correr','darse cuenta','dar','encontrar']\n",
    "\n",
    "NUMBER_OF_PERSONS = 10\n",
    "NUMBER_OF_VIDEOS_PER_PERSON = 5\n",
    "\n",
    "frame_columns = []\n",
    "\n",
    "AMOUNT_OF_FRAMES = 10\n",
    "# amount_of_frames = 30\n",
    "\n",
    "FACE_POINTS = 468\n",
    "posePointIndexes = [k for k in range(23)] #de los 33 puntos solo tomamos los hombros, brazos, cabeza (SIN CINTURA debido a los videos del set de datos)\n",
    "LEFT_HAND_POINTS = 21\n",
    "RIGHT_HAND_POINTS = 21\n",
    "\n",
    "# CAMBIAR ESTE BOOLEANO PARA TOMAR PUNTOS FACIALES O NO (se tomar치n CEJAS y BOCA)\n",
    "USE_FACE_POINTS = False\n",
    "\n",
    "# https://github.com/tensorflow/tfjs-models/commit/838611c02f51159afdd77469ce67f0e26b7bbb23#diff-e5d31503f11c6bae62542ea89982152514b81906dff0b718e44708bcf22aa361\n",
    "# https://github.com/ManuelTS/augmentedFaceMeshIndices/blob/master/Left_Eye.jpg\n",
    "# https://github.com/google/mediapipe/blob/a908d668c730da128dfa8d9f6bd25d519d006692/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\n",
    "\n",
    "# No he repetido los puntos que tenian en comun ciertos arreglos, como los puntos faciales en los datos\n",
    "# obtenidos de los videos se encuentran primeros, estos indices se corresponden\n",
    "\n",
    "rightEyebrowUpper = [156, 70, 63, 105, 66, 107, 55, 193]\n",
    "rightEyebrowLower = [35, 124, 46, 53, 52, 65]\n",
    "\n",
    "leftEyebrowUpper  = [383, 300, 293, 334, 296, 336, 285, 417]\n",
    "leftEyebrowLower  = [265, 353, 276, 283, 282, 295]\n",
    "    \n",
    "lipsUpperOuter    = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]\n",
    "lipsLowerOuter    = [146, 91, 181, 84, 17, 314, 405, 321, 375]\n",
    "lipsUpperInner    = [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308]\n",
    "lipsLowerInner    = [95, 88, 178, 87, 14, 317, 402, 318, 324]\n",
    "\n",
    "facePointsIndexes = rightEyebrowUpper + rightEyebrowLower + leftEyebrowUpper + leftEyebrowLower + lipsUpperOuter + lipsLowerOuter + lipsUpperInner + lipsLowerInner\n",
    "\n",
    "for frame in range(AMOUNT_OF_FRAMES):\n",
    "    \n",
    "    if (USE_FACE_POINTS):\n",
    "        for index in range (len(facePointsIndexes)):\n",
    "            frame_columns.append(f'''fr_{frame}_face_p{index}_x''')\n",
    "            frame_columns.append(f'''fr_{frame}_face_p{index}_y''')\n",
    "    \n",
    "    for index in range (len(posePointIndexes)):\n",
    "        frame_columns.append(f'''fr_{frame}_pose_p{index}_x''')\n",
    "        frame_columns.append(f'''fr_{frame}_pose_p{index}_y''')\n",
    "\n",
    "    for index in range (LEFT_HAND_POINTS):\n",
    "        frame_columns.append(f'''fr_{frame}_left_hand_p{index}_x''')\n",
    "        frame_columns.append(f'''fr_{frame}_left_hand_p{index}_y''')\n",
    "\n",
    "    for index in range (RIGHT_HAND_POINTS):\n",
    "        frame_columns.append(f'''fr_{frame}_right_hand_p{index}_x''')\n",
    "        frame_columns.append(f'''fr_{frame}_right_hand_p{index}_y''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)                 #prediction from a frame\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "# COLORES PARA PUNTOS Y LINEAS A DIBUJAR EN LA IMAGEN\n",
    "leftHandStyles = [0,138,255,2,1] #naranja\n",
    "rightHandStyles = [231,217,0,2,1] #celeste\n",
    "faceStyles = [80,110,10,0,1]\n",
    "poseStyles = [70,100,5,2,1]\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    #     FACE\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        mp_drawing.DrawingSpec(color=(faceStyles[0],faceStyles[1],faceStyles[2]), thickness=faceStyles[3], circle_radius=faceStyles[4])\n",
    "    )\n",
    "    #     POSE/BODY\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(poseStyles[0],poseStyles[1],poseStyles[2]), thickness=poseStyles[3], circle_radius=poseStyles[4])\n",
    "    )\n",
    "    #     LEFT HAND\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.left_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(leftHandStyles[0],leftHandStyles[1],leftHandStyles[2]), thickness=leftHandStyles[3], circle_radius=leftHandStyles[4])\n",
    "    )\n",
    "    #     RIGHT HAND\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.right_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(rightHandStyles[0],rightHandStyles[1],rightHandStyles[2]), thickness=rightHandStyles[3], circle_radius=rightHandStyles[4])\n",
    "    )\n",
    "\n",
    "def array_from_landmarks(results):\n",
    "    # LEFT HAND array\n",
    "    # len(results.left_hand_landmarks.landmark) = 21 landmarks for each hand, with 2 coordinates each landmark\n",
    "    if (results.left_hand_landmarks):\n",
    "        leftHandLandmarks = np.array([[result.x, result.y] for result in results.left_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        leftHandLandmarks = np.zeros(21*2)\n",
    "\n",
    "\n",
    "    # RIGHT HAND array\n",
    "    # len(results.right_hand_landmarks.landmark) -> same for right hand\n",
    "    if (results.right_hand_landmarks):\n",
    "        rightHandLandmarks = np.array([[result.x, result.y ] for result in results.right_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        rightHandLandmarks = np.zeros(21*2)\n",
    "\n",
    "\n",
    "    # POSE array\n",
    "    # len(results.pose_landmarks.landmark) -> 33 landmarks of 2 coordinates each one (X, Y)\n",
    "    if (results.pose_landmarks):\n",
    "        poseLandmarks = np.array([[result.x, result.y] for result in results.pose_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        poseLandmarks = np.zeros(33*2)\n",
    "\n",
    "\n",
    "    # FACE array\n",
    "    # len(results.face_landmarks.landmark) -> 468 landmarks of 2 coordinates each one\n",
    "    if (results.face_landmarks):\n",
    "        faceLandmarks = np.array([[result.x, result.y] for result in results.face_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        faceLandmarks = np.zeros(468*2)\n",
    "        \n",
    "    frame = np.concatenate((faceLandmarks, poseLandmarks, leftHandLandmarks, rightHandLandmarks))\n",
    "    \n",
    "    values_validation(frame)\n",
    "    \n",
    "    aux_array = []\n",
    "    #CARA\n",
    "    if (USE_FACE_POINTS):\n",
    "        for faceKeypoint in facePointsIndexes:\n",
    "            aux_array.append(frame[faceKeypoint*2])     # X\n",
    "            aux_array.append(frame[faceKeypoint*2+1])   # Y\n",
    "    #POSE\n",
    "    for poseKeypoint in range(936, 936+len(posePointIndexes)*2):\n",
    "        aux_array.append(frame[poseKeypoint])\n",
    "    #MANOS\n",
    "    for keypoint in range(1002, 1086):\n",
    "        aux_array.append(frame[keypoint])\n",
    "                \n",
    "    return aux_array\n",
    "\n",
    "def values_validation(frame):\n",
    "    for i in range(len(frame)):\n",
    "        if (frame[i] > 1):\n",
    "            frame[i] = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRUEBA CON CAMARA EN TIEMPO REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = cv2.VideoCapture(0)\n",
    "caption.set(cv2.CAP_PROP_FPS, 60)\n",
    "resultsForVideo = []\n",
    "\n",
    "framePointsLength = int(len(frame_columns)/AMOUNT_OF_FRAMES)\n",
    "sentence = \"\"\n",
    "i = 0\n",
    "with mp_holistic.Holistic(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as holisticModel:\n",
    "    framesWindow = []\n",
    "    while caption.isOpened():\n",
    "        ret, frame = caption.read()\n",
    "        i = i + 1\n",
    "        image, results = mediapipe_detection(frame, holisticModel)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = array_from_landmarks(results)\n",
    "        \n",
    "        # SIN SALTO DE FRAMES\n",
    "        framesWindow.extend(keypoints)\n",
    "\n",
    "        #HACE FUNCION DE VENTANA DESLIZANTE DE FRAMES\n",
    "        if (len(framesWindow) > len(frame_columns)):\n",
    "            framesWindow = framesWindow[framePointsLength:]\n",
    "\n",
    "        # PREDICE AL LLEGAR A 10 FRAMES\n",
    "        if (len(framesWindow) >= len(frame_columns)):\n",
    "            #if (lastFrameAdded == i - 1):\n",
    "                #  res = model.predict(np.expand_dims(keypoints, axis=0))\n",
    "            res = model.predict([framesWindow])\n",
    "            resultsForVideo.append(res[0])\n",
    "            \n",
    "            sentence = SIGNS_LIST[res[0]]\n",
    "        \n",
    "        cv2.putText(image, ' '.join(sentence), (10,20), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,255,255), 2)\n",
    "        cv2.putText(image, 'Frame 춿: ' + str(i), (10,50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,255,255), 2)\n",
    "        cv2.imshow('Predicting sign..', image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'): #press 'q' to break\n",
    "            break\n",
    "            \n",
    "    caption.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRUEBA DESDE VIDEOS\n",
    "\n",
    "Para la lista de se침as se procesar치n los videos, de ser necesario, se puede comentar los 3 bucles (for) y colocar el nombre/ruta del video especifico a procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/LSA64/all_cut/005_001_001.mp4\n",
      "Se acab칩\n"
     ]
    }
   ],
   "source": [
    "resultsIndexes = []\n",
    "indexAndName = []\n",
    "framePointsLength = int(len(frame_columns)/AMOUNT_OF_FRAMES)\n",
    "lastFrameAdded = 0\n",
    "with mp_holistic.Holistic(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as holisticModel:\n",
    "    for sign in range(len(FULL_SIGNS_LIST)):\n",
    "        if (SIGNS_LIST.count(FULL_SIGNS_LIST[sign])):\n",
    "            for j in range(NUMBER_OF_PERSONS):\n",
    "                for k in range(NUMBER_OF_VIDEOS_PER_PERSON):\n",
    "                    caption = cv2.VideoCapture(f'''C:/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/LSA64/all_cut/0{str(sign+1).zfill(2)}_0{str(j+1).zfill(2)}_00{k+1}.mp4''')\n",
    "                    print(f'''C:/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/LSA64/all_cut/0{str(sign+1).zfill(2)}_0{str(j+1).zfill(2)}_00{k+1}.mp4''')\n",
    "                    framesLength = int(caption.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                    i = 0\n",
    "                    \n",
    "                    framesWindow = []\n",
    "                    resultsForVideo = []\n",
    "                    while (caption.isOpened()):\n",
    "                        ret, frame = caption.read()\n",
    "                        if (ret):\n",
    "                            image, results = mediapipe_detection(frame, holisticModel)\n",
    "                            draw_styled_landmarks(image, results)\n",
    "                            keypoints = array_from_landmarks(results)\n",
    "                            \n",
    "                            # SE PUEDE PROBAR SALTEANDO/SKIPPEANDO FRAMES DEL VIDEO (== pares) (!= impares)\n",
    "                            # if (i % 2 == 0):\n",
    "                                # framesWindow.extend(keypoints)\n",
    "                                # lastFrameAdded = i\n",
    "                                # i = i + 1\n",
    "                            \n",
    "                            # SIN SALTO DE FRAMES\n",
    "                            framesWindow.extend(keypoints)\n",
    "                            \n",
    "                            #HACE FUNCION DE VENTANA DESLIZANTE DE FRAMES\n",
    "                            if (len(framesWindow) > len(frame_columns)):\n",
    "                                framesWindow = framesWindow[framePointsLength:]\n",
    "                            \n",
    "                            # PREDICE AL LLEGAR A 10 FRAMES\n",
    "                            if (len(framesWindow) >= len(frame_columns)):\n",
    "                                # if (lastFrameAdded == i - 1):\n",
    "                                #  res = model.predict(np.expand_dims(keypoints, axis=0))\n",
    "                                res = model.predict([framesWindow])\n",
    "                                resultsForVideo.append(res[0])\n",
    "                                # resultsIndexes.append(res[0])\n",
    "                            cv2.imshow('Predicting sign..', image)\n",
    "                        else:\n",
    "                            print(\"Se acab칩\")\n",
    "                            resultsIndexes.append(resultsForVideo)\n",
    "                            indexAndName.append({'name': FULL_SIGNS_LIST[sign], 'index': SIGNS_LIST.index(FULL_SIGNS_LIST[sign])})\n",
    "                            break\n",
    "                        if cv2.waitKey(10) & 0xFF == ord('q'): #press 'q' to break\n",
    "                            break\n",
    "                    caption.release()\n",
    "                    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 15, 15, 18, 18]\n",
      "4\n",
      "0::56\n",
      "{'2': 51, '6': 1, '15': 2, '18': 2}\n"
     ]
    }
   ],
   "source": [
    "for results in range(len(resultsIndexes)):\n",
    "    resultsIndexes[results].sort()\n",
    "    dicts = {}\n",
    "    for element in resultsIndexes[results]:\n",
    "        element = str(element)\n",
    "        if(element in dicts):\n",
    "            dicts[element] = dicts[element] + 1\n",
    "        else:\n",
    "            dicts[element] = 1\n",
    "    print(resultsIndexes[results]) # Muestra el arreglo de predicciones realizadas para el video ordenadas por clase ascendentemente\n",
    "    print(indexAndName[results]['index']) #Muestra el indice de la palabra en el arreglo SIGNS_LIST\n",
    "    # Muestra-compara la cantidad de predicciones correctas contra el total de predicciones realizadas\n",
    "    print(str(resultsIndexes[results].count(indexAndName[results]['index']))+'::'+str(len(resultsIndexes[results])))\n",
    "    print(dicts) # Muestra diccionario de clases predichas y su cantidad para el video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
