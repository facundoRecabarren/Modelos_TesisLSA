{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de dataframe/parquet con datos de todos los videos\n",
    "dataframe = pd.read_parquet('../01_datos/00_raw/raw_parquet')\n",
    "\n",
    "\"\"\" \n",
    "    ARMADO DE COLUMNAS PARA LECTURA DE DATAFRAME \n",
    "    Columnas numericas del 0 a 1085 representando la cantidad de puntos totales.\n",
    "\"\"\"\n",
    "columnsToRead = [str(i) for i in range(1086)]\n",
    "columnsToRead.append('sign')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A TENER EN CUENTA:\n",
    "\n",
    "<span style=\"color:red\">\n",
    "CAMBIAR EL ORDEN DE LAS PALABRAS EN ALGUNA DE LAS 2 LISTAS VA A LLEVAR A UNA LECTURA INCORRECTA DE LOS DATOS \n",
    "\n",
    "EN EL PARQUET COMPLETO ORIGINAL SI SE REALIZA UN CAMBIO ESTE DEBE REALIZARSE SOBRE LAS DEMAS NOTEBOOK DE\n",
    "\n",
    "LECTURA Y CREACION DE PARQUETS YA SEA SOBRE EL ORIGINAL O LOS DE MUESTREO\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNS_LIST = ['nacer','comida','brillante','mujer', 'hijo', 'hombre', 'lejos', 'aprender', 'espumadera','amargo','leche','Uruguay','pais','donde','ninguno','nombre','perfume','sordo','comprar','encontrar', 'nave espacial']\n",
    "\n",
    "# se toma la lista original de señas ya que estas en el orden en que se encuentran, su indice representa correctamente el nombre del video original\n",
    "FULL_SIGNS_LIST = ['opaco', 'rojo', 'verde', 'amarillo', 'brillante', 'celeste', 'colores', 'rosa', 'mujer', 'enemigo', 'hijo', 'hombre', 'lejos','cajón','nacer','aprender','llamar','espumadera','amargo','dulce','leche','agua','comida','Argentina','Uruguay','pais','donde','apellido','burla','cumpleanos','desayuno','foto','hambre','mapa','moneda','musica','nave espacial','ninguno','nombre','paciencia','perfume','sordo','trampa','arroz','asado','caramelo','chicle','fideos','yogurt','aceptar','agradecer','apagar','aparecer','aterrizar','atrapar','ayudar','bailar','bañarse','comprar','copiar','correr','darse cuenta','dar','encontrar']\n",
    "\n",
    "NUMBER_OF_PERSONS = 10\n",
    "NUMBER_OF_VIDEOS_PER_PERSON = 5\n",
    "AMOUNT_OF_FRAMES = 10\n",
    "# semilla para la generacion de numero aleatorios en la eleccion de frames\n",
    "RANDOM_SEED = 48"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ACTIVACION/DESACTIVACION DE CARA y CANTIDAD DE FRAMES\n",
    "La variable \"useFacePoints\" añadirá las columnas necesarias para los puntos faciales que serán añadidos en el parquet a construir.\n",
    "La variable \"amount_of_frames\" aumentará las cantidad de columnas necesarias, en este caso, cuantos frames se tomaran de cada video para construir el parquet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_columns = []\n",
    "\n",
    "FACE_POINTS = 468\n",
    "POSE_POINT_INDEXES = [k for k in range(23)] #de los 33 puntos solo tomamos los hombros, brazos, cabeza (SIN CINTURA debido a los videos del set de datos)\n",
    "LEFT_HAND_POINTS = 21\n",
    "RIGHT_HAND_POINTS = 21\n",
    "\n",
    "# CAMBIAR ESTE BOOLEANO PARA TOMAR PUNTOS FACIALES O NO (se tomarán CEJAS y BOCA)\n",
    "USE_FACE_POINTS = False\n",
    "\n",
    "# https://github.com/tensorflow/tfjs-models/commit/838611c02f51159afdd77469ce67f0e26b7bbb23#diff-e5d31503f11c6bae62542ea89982152514b81906dff0b718e44708bcf22aa361\n",
    "# https://github.com/ManuelTS/augmentedFaceMeshIndices/blob/master/Left_Eye.jpg\n",
    "# https://github.com/google/mediapipe/blob/a908d668c730da128dfa8d9f6bd25d519d006692/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\n",
    "\n",
    "# No he repetido los puntos que tenian en comun ciertos arreglos, como los puntos faciales en los datos\n",
    "# obtenidos de los videos se encuentran primeros, estos indices se corresponden\n",
    "\n",
    "rightEyebrowUpper = [156, 70, 63, 105, 66, 107, 55, 193]\n",
    "rightEyebrowLower = [35, 124, 46, 53, 52, 65]\n",
    "\n",
    "leftEyebrowUpper  = [383, 300, 293, 334, 296, 336, 285, 417]\n",
    "leftEyebrowLower  = [265, 353, 276, 283, 282, 295]\n",
    "    \n",
    "lipsUpperOuter    = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]\n",
    "lipsLowerOuter    = [146, 91, 181, 84, 17, 314, 405, 321, 375]\n",
    "lipsUpperInner    = [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308]\n",
    "lipsLowerInner    = [95, 88, 178, 87, 14, 317, 402, 318, 324]\n",
    "\n",
    "facePointsIndexes = rightEyebrowUpper + rightEyebrowLower + leftEyebrowUpper + leftEyebrowLower + lipsUpperOuter + lipsLowerOuter + lipsUpperInner + lipsLowerInner\n",
    "\n",
    "# print(\"Puntos de cara \"+str(len(facePointsIndexes)))\n",
    "# print(\"Puntos de Posicion \"+str(len(POSE_POINT_INDEXES)))\n",
    "\n",
    "def build_columns():\n",
    "    print('Frames: ' + str(AMOUNT_OF_FRAMES))\n",
    "    print('Usar puntos faciales: ' + str(USE_FACE_POINTS))\n",
    "    global frame_columns\n",
    "    frame_columns = []\n",
    "\n",
    "    for frame in range(AMOUNT_OF_FRAMES):\n",
    "        \n",
    "        if (USE_FACE_POINTS):\n",
    "            for index in range (len(facePointsIndexes)):\n",
    "                frame_columns.append(f'''fr_{frame}_face_p{index}_x''')\n",
    "                frame_columns.append(f'''fr_{frame}_face_p{index}_y''')\n",
    "        \n",
    "        for index in range (len(POSE_POINT_INDEXES)):\n",
    "            frame_columns.append(f'''fr_{frame}_pose_p{index}_x''')\n",
    "            frame_columns.append(f'''fr_{frame}_pose_p{index}_y''')\n",
    "\n",
    "        for index in range (LEFT_HAND_POINTS):\n",
    "            frame_columns.append(f'''fr_{frame}_left_hand_p{index}_x''')\n",
    "            frame_columns.append(f'''fr_{frame}_left_hand_p{index}_y''')\n",
    "\n",
    "        for index in range (RIGHT_HAND_POINTS):\n",
    "            frame_columns.append(f'''fr_{frame}_right_hand_p{index}_x''')\n",
    "            frame_columns.append(f'''fr_{frame}_right_hand_p{index}_y''')\n",
    "    frame_columns.append(\"sign\")\n",
    "    print('Cantidad de columnas: '+ str(len(frame_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_validation(frame):\n",
    "    for i in range(len(frame)):\n",
    "        if (frame[i] > 1):\n",
    "            if (frame[i] > 2.8): \n",
    "                print(\"Valor que supera al 2.8: \"+ str(frame[i]))\n",
    "            frame[i] = 1\n",
    "    \n",
    "def getVideoKeypoints(sign_iteration, person, video):\n",
    "    frames = []\n",
    "\n",
    "    ''' El valor 201 corresponde a la longitud fija que se otorgó a todos los videos\n",
    "    ya sea que estos la cumplan o no. Al momento de armar el parquet \n",
    "    con todos los videos procesados, aquellos que no llegaban a esta longitud se\n",
    "    completaron con \"frames\" de valores 3 (valor que no debería existir en los datos \n",
    "    originales por la normalizacion que realiza MediaPipe)\n",
    "    El valor 50 proviene de la cantidad de videos para cada seña\n",
    "    \n",
    "    Bloque de Seña: la seña me indica el bloque, y como cada bloque está compuesto por 50videos por 201 filas cada 1, el salto de bloque me lo da el indice de la seña que se está procesando comenzando desde el 0.\n",
    "    Bloque de seña: 50*201*sign_index\n",
    "\n",
    "    Bloque de persona: cada persona filma 5 videos por seña y como tenemos 10 personas, para acceder a la correspondiente realizamos, 5 videos de la persona * 201 frames por cada video\n",
    "    Bloque de persona: indiceDePersona * 5 * 201\n",
    "\n",
    "    Bloque de video: Una vez que conocemos el Bloque de Seña y Bloque de persona queda posicionar el video. Cada video esta compuesto por 201 frames\n",
    "    Bloque de video: 201*indiceDeVideo\n",
    "    '''\n",
    "    for frameIndex in range(201):\n",
    "        frame = []\n",
    "        frame = dataframe.iloc[ (50*201*sign_iteration)+(person*5*201)+(201*video) + frameIndex ].to_list()\n",
    "        # quito ultima columna con la seña de cada frame/fila\n",
    "        frame.pop()\n",
    "\n",
    "        frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "def store_dataset(dataset, name):\n",
    "    dataset.to_parquet('./01_criterios/{0}.parquet'.format(name))\n",
    "    print('Se almacenó: ./01_criterios/{0}.parquet'.format(name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUESTREO RANDOM\n",
    "Se toman frames aleatorios entre los indices:\n",
    "\n",
    "    [0, cantidad de Frames de video correspondiente]\n",
    "\n",
    "Los números aleatorios tomados dentro de este intervalo se ordenan ascendentemente para mantener la secuencia del video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "    dataFrameRows = []\n",
    "\n",
    "    # sign_iteration se utiliza para poder acceder a los bloques de la seña correspondiente\n",
    "    # por ejemplo la seña 'Brillante' se encontrará primera en el parquet pero con la lista de señas no es posible conocer su posicion en el parquet\n",
    "    # debido a esto se utiliza esta variable\n",
    "    sign_iteration = 0\n",
    "\n",
    "    random.seed(RANDOM_SEED)\n",
    "\n",
    "    for sign in range(len(FULL_SIGNS_LIST)):\n",
    "        if (SIGNS_LIST.count(FULL_SIGNS_LIST[sign])):\n",
    "            for j in range(10):\n",
    "                for k in range(5):\n",
    "                    cap = cv2.VideoCapture(f'''C:/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/LSA64/all_cut/0{str(sign+1).zfill(2)}_0{str(j+1).zfill(2)}_00{k+1}.mp4''')\n",
    "                    framesLength = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "                    video_keypoints = getVideoKeypoints(sign_iteration = sign_iteration, person = j, video = k)\n",
    "\n",
    "                    randomFrames = []\n",
    "                    for i in range(AMOUNT_OF_FRAMES):\n",
    "                        randomFrames.append(random.randint(0, framesLength-1))\n",
    "                    randomFrames.sort()\n",
    "\n",
    "                    aux_array = []\n",
    "                    for randomFrame in randomFrames:\n",
    "                        values_validation(video_keypoints[randomFrame])\n",
    "                        # CARA\n",
    "                        if (USE_FACE_POINTS):\n",
    "                            for faceKeypoint in facePointsIndexes:\n",
    "                                aux_array.append(video_keypoints[randomFrame][faceKeypoint*2])     # X\n",
    "                                aux_array.append(video_keypoints[randomFrame][faceKeypoint*2+1])   # Y\n",
    "                        # POSE\n",
    "                        for poseKeypoint in range(936, 936+len(POSE_POINT_INDEXES)*2):\n",
    "                            aux_array.append(video_keypoints[randomFrame][poseKeypoint])\n",
    "                        # MANOS\n",
    "                        for keypoint in range(1002, 1086):\n",
    "                            aux_array.append(video_keypoints[randomFrame][keypoint])\n",
    "                    aux_array.append(FULL_SIGNS_LIST[sign])\n",
    "\n",
    "                    dataFrameRows.append(aux_array)\n",
    "            sign_iteration = sign_iteration + 1\n",
    "                \n",
    "    return pd.DataFrame(dataFrameRows,columns=frame_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_extended_dataset():\n",
    "    dataFrameRows = []\n",
    "\n",
    "    # sign_iteration se utiliza para poder acceder a los bloques de la seña correspondiente\n",
    "    # por ejemplo la seña 'Brillante' se encontrará primera en el parquet pero con la lista de señas no es posible conocer su posicion en el parquet\n",
    "    # debido a esto se utiliza esta variable\n",
    "    sign_iteration = 0\n",
    "\n",
    "    random.seed(RANDOM_SEED)\n",
    "\n",
    "    for sign in range(len(FULL_SIGNS_LIST)):\n",
    "        if (SIGNS_LIST.count(FULL_SIGNS_LIST[sign])):\n",
    "            for j in range(10):\n",
    "                for k in range(5):\n",
    "                    cap = cv2.VideoCapture(f'''C:/Users/facur/Desktop/tesis_LSA/codigos_datos_tesis/LSA64/all_cut/0{str(sign+1).zfill(2)}_0{str(j+1).zfill(2)}_00{k+1}.mp4''')\n",
    "                    framesLength = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "                    video_keypoints = getVideoKeypoints(sign_iteration = sign_iteration, person = j, video = k)\n",
    "\n",
    "                    randomFrames = []\n",
    "                    for i in range(AMOUNT_OF_FRAMES):\n",
    "                        randomFrames.append(random.randint(0, framesLength-1))\n",
    "                    randomFrames.sort()\n",
    "\n",
    "                    aux_array = []\n",
    "                    for randomFrame in randomFrames:\n",
    "                        values_validation(video_keypoints[randomFrame])\n",
    "                        if (USE_FACE_POINTS):\n",
    "                            for faceKeypoint in facePointsIndexes:\n",
    "                                aux_array.append(video_keypoints[randomFrame][faceKeypoint*2])     # X\n",
    "                                aux_array.append(video_keypoints[randomFrame][faceKeypoint*2+1])   # Y\n",
    "                        # POSE\n",
    "                        for poseKeypoint in range(936, 936+len(POSE_POINT_INDEXES)*2):\n",
    "                            aux_array.append(video_keypoints[randomFrame][poseKeypoint])\n",
    "                        for keypoint in range(1002, 1086):\n",
    "                            aux_array.append(video_keypoints[randomFrame][keypoint])\n",
    "                    aux_array.append(FULL_SIGNS_LIST[sign])\n",
    "\n",
    "                    dataFrameRows.append(aux_array)\n",
    "            sign_iteration = sign_iteration + 1\n",
    "                \n",
    "    return pd.DataFrame(dataFrameRows,columns=frame_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames: 10\n",
      "Usar puntos faciales: False\n",
      "Cantidad de columnas: 1301\n",
      "Nro de Filas: 1050\n",
      "Nro de Columnas: 1301\n",
      "Se almacenó: ./01_criterios/criterio_random_10fr_no_face.parquet\n",
      "Frames: 10\n",
      "Usar puntos faciales: True\n",
      "Cantidad de columnas: 2661\n",
      "Nro de Filas: 1050\n",
      "Nro de Columnas: 2661\n",
      "Se almacenó: ./01_criterios/criterio_random_10fr_with_face.parquet\n",
      "Frames: 30\n",
      "Usar puntos faciales: False\n",
      "Cantidad de columnas: 3901\n",
      "Nro de Filas: 1050\n",
      "Nro de Columnas: 3901\n",
      "Se almacenó: ./01_criterios/criterio_random_30fr_no_face.parquet\n",
      "Frames: 30\n",
      "Usar puntos faciales: True\n",
      "Cantidad de columnas: 7981\n",
      "Nro de Filas: 1050\n",
      "Nro de Columnas: 7981\n",
      "Se almacenó: ./01_criterios/criterio_random_30fr_with_face.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------- DATASET muestreo 10 frames SIN cara\n",
    "AMOUNT_OF_FRAMES = 10\n",
    "USE_FACE_POINTS = False\n",
    "\n",
    "build_columns()\n",
    "dataset = build_dataset()\n",
    "print('Nro de Filas: '+str(len(dataset.axes[0])))\n",
    "print('Nro de Columnas: '+str(len(dataset.axes[1])))\n",
    "# dataset.head()\n",
    "store_dataset(dataset, 'criterio_random_10fr_no_face')\n",
    "\n",
    "# -------------- DATASET muestreo 10 frames CON cara\n",
    "AMOUNT_OF_FRAMES = 10\n",
    "USE_FACE_POINTS = True\n",
    "\n",
    "build_columns()\n",
    "dataset = build_dataset()\n",
    "print('Nro de Filas: '+str(len(dataset.axes[0])))\n",
    "print('Nro de Columnas: '+str(len(dataset.axes[1])))\n",
    "# dataset.head()\n",
    "store_dataset(dataset, 'criterio_random_10fr_with_face')\n",
    "\n",
    "# -------------- DATASET muestreo 30 frames SIN cara\n",
    "AMOUNT_OF_FRAMES = 30\n",
    "USE_FACE_POINTS = False\n",
    "\n",
    "build_columns()\n",
    "dataset = build_extended_dataset()\n",
    "print('Nro de Filas: '+str(len(dataset.axes[0])))\n",
    "print('Nro de Columnas: '+str(len(dataset.axes[1])))\n",
    "# dataset.head()\n",
    "store_dataset(dataset, 'criterio_random_30fr_no_face')\n",
    "\n",
    "# -------------- DATASET muestreo 30 frames CON cara\n",
    "AMOUNT_OF_FRAMES = 30\n",
    "USE_FACE_POINTS = True\n",
    "\n",
    "build_columns()\n",
    "dataset = build_extended_dataset()\n",
    "print('Nro de Filas: '+str(len(dataset.axes[0])))\n",
    "print('Nro de Columnas: '+str(len(dataset.axes[1])))\n",
    "# dataset.head()\n",
    "store_dataset(dataset, 'criterio_random_30fr_with_face')\n",
    "\n",
    "del dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
