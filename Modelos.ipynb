{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c69fc4",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea93fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking, GRU, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping #,TensorBoard\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c9f4a",
   "metadata": {},
   "source": [
    "# Datos Necesarios para los Modelos\n",
    "\n",
    "- Definicion de: Path/Ruta de alamcenamiento, Lista de Palabras, Cantidad de Personas, Cantidad de Videos por Persona y numero maximo de Frames.\n",
    "- Armado de DICCIONARIO que mapea VALOR(seña/palabra) a -> indice/número\n",
    "- Se almacenan en 2 variables los datos con los que se entrenaran los modelos \"sequences\" almacena en un arreglo los datos de cada video, \"labels\" es un arreglo de 0s y 1s que indica la clase a la que pertenece cada video. Esta celda puede ser reemplazada por la del apartado Limpieza de Datos (al final), el procesamiento es el mismo pero se tiene en cuenta \"frames sucios\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a43c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE_PATH2 = os.path.join('LSA64_data')\n",
    "\n",
    "# Lista de Palabras con las que se armarán las oraciones, no se encuentran las 64 palabras\n",
    "signs_list = ['opaco', 'rojo', 'verde', 'amarillo', 'brillante', 'celeste', 'colores', 'rosa', 'mujer', 'enemigo', 'hijo', 'hombre', 'lejos','cajón','aprender','llamar','espumadera','amargo','dulce','leche','agua','comida','Argentina','Uruguay','pais','donde','apellido','burla','cumpleanos','desayuno','foto','hambre','mapa','moneda','musica','nave espacial','ninguno','nombre','paciencia','perfume','sordo','trampa','arroz','asado','caramelo','chicle','fideos','yogurt','aceptar','agradecer','apagar','aparecer','aterrizar','atrapar','ayudar','bailar','bañarse','comprar','copiar','correr','darse cuenta','dar','encontrar']\n",
    "# signs_list = ['nacer','comida','brillante', 'mujer', 'hijo', 'hombre', 'lejos', 'aprender', 'espumadera','amargo','leche','Uruguay','pais','donde','ninguno','nombre','perfume','sordo','comprar','encontrar', 'nave espacial']\n",
    "numberOfPersons = 10\n",
    "numberOfVideosPerPerson = 5\n",
    "\n",
    "maximumNumberOfFrames = 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63fb7848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nacer': 0, 'comida': 1, 'brillante': 2, 'mujer': 3, 'hijo': 4, 'hombre': 5, 'lejos': 6, 'aprender': 7, 'espumadera': 8, 'amargo': 9, 'leche': 10, 'Uruguay': 11, 'pais': 12, 'donde': 13, 'ninguno': 14, 'nombre': 15, 'perfume': 16, 'sordo': 17, 'comprar': 18, 'encontrar': 19, 'nave espacial': 20}\n"
     ]
    }
   ],
   "source": [
    "# DICCIONARIO que mapea VALOR(seña/palabra) a -> indice/número\n",
    "sign_labels = {label : index for index, label in enumerate(signs_list)}\n",
    "print(sign_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5edca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LA EJECUCION DE ESTA CELDA PUEDE SER REEMPLAZADA POR LA DEL APARTADO LIMPIEZA DE DATOS\n",
    "sequences, labels = [],[]\n",
    "for sign in range(len(signs_list)):\n",
    "    for personNumber in range(numberOfPersons-1):\n",
    "        for videoNumber in range(numberOfVideosPerPerson):\n",
    "            result = np.load(os.path.join(STORE_PATH2, signs_list[sign], f'''0{str(personNumber)}_0{str(videoNumber)}.npy'''))\n",
    "            resultWithoutFace = []\n",
    "# salto de a 1 frame para simular un \"muestreo\" de cada video y no usar todos los frames, no es un muestreo probabilistico\n",
    "            for index in range(0,201,2):\n",
    "#                 NO TOMO EN CUENTA LOS DATOS DE LA CARA\n",
    "                resultWithoutFace.append(result[index][936:])\n",
    "            sequences.append(resultWithoutFace)\n",
    "            labels.append(sign_labels[signs_list[sign]])\n",
    "            \n",
    "sequences = np.array(sequences)\n",
    "labels = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf02ea",
   "metadata": {},
   "source": [
    "# DEFINICIÓN DE MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e86f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Masking(mask_value=paddingValue, input_shape=(201, 1086)))\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(40, 1086)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dense(np.array(signs_list).shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6245909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUARTO MODELO USADO GRU - GRU\n",
    "paddingValue = 3\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=paddingValue, input_shape=(201, 1086)))\n",
    "model.add(GRU(128, return_sequences=True, activation='tanh'))\n",
    "model.add(GRU(128, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(np.array(signs_list).shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13dd2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUINTO MODELO USADO\n",
    "paddingValue = 3\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=paddingValue, input_shape=(201, 1086)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
    "model.add(LSTM(128, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dense(np.array(signs_list).shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13bb67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paddingValue = 3\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=paddingValue, input_shape=(201, 150)))\n",
    "model.add(GRU(128, return_sequences=True, activation='tanh'))\n",
    "model.add(GRU(128, return_sequences=True, activation='tanh'))\n",
    "model.add(GRU(64, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(np.array(signs_list).shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30/11 pruebo para ver si con LSTM tambien demora 5 segundos sin datos de cara\n",
    "paddingValue = 3\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=paddingValue, input_shape=(201, 150)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(128, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dense(np.array(signs_list).shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a713682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armo un modelo para entrenar habiendo armado los arreglos con un salto de frames (sampleo o muestreo)\n",
    "paddingValue = 3\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=paddingValue, input_shape=(101, 150)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(128, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dense(np.array(signs_list).shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ae338",
   "metadata": {},
   "source": [
    "# COMPILACIÓN DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d84b7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "esLoss = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=30, min_delta=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31d2ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(sequences, labels , test_size=0.05, random_state=1)\n",
    "\n",
    "model.fit(xTrain, yTrain, validation_data=(xTest,yTest), epochs=500, callbacks=[esLoss])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef597a63",
   "metadata": {},
   "source": [
    "# EVALUACION DEL MODELO\n",
    "\n",
    "- Se evalua el modelo entrenado sobre todos los videos\n",
    "- Se evalua el modelo sobre los datos que se separaron para Testeo (xTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluo las predicciones del modelo sobre TODO el set de datos de los videos procesados\n",
    "threshold08 = 0.8\n",
    "threshold09 = 0.9\n",
    "\n",
    "totalProcessed = 0\n",
    "\n",
    "predictedOkay08 = 0\n",
    "predictedOkay09 = 0\n",
    "\n",
    "for sign in range(len(signs_list)):\n",
    "    for personNumber in range(numberOfPersons):\n",
    "        for videoNumber in range(numberOfVideosPerPerson):\n",
    "            videoData = np.load(os.path.join(STORE_PATH2, signs_list[sign], f'''0{str(personNumber)}_0{str(videoNumber)}.npy'''))\n",
    "            # resultWithDistances = []\n",
    "            # for index in range(201):\n",
    "            #   resultWithDistances.append(getEuclideanDistances(videoData[index]))\n",
    "            # print(f'''VIDEO: LSA64_data/{signs_list[sign]}/0{personNumber}_0{videoNumber}''')\n",
    "            # videoData2 = []\n",
    "            # for index in range(201):\n",
    "                # if (index % 2 != 0):\n",
    "                    # videoData2.append(videoData[index])\n",
    "            resultWithoutFace = []\n",
    "            for index in range(0,201,2):\n",
    "                resultWithoutFace.append(videoData[index][936:])\n",
    "#                 resultWithoutFace.append(videoData[index])\n",
    "            response = model.predict(np.expand_dims(resultWithoutFace, axis=0))\n",
    "            if (np.max(response) > threshold08):\n",
    "#                 print('Seña leída: '+ signs_list[sign] + '------  Predice: ' + signs_list[np.argmax(response)])\n",
    "                if (signs_list[sign] == signs_list[np.argmax(response)]):\n",
    "                    predictedOkay08 = predictedOkay08 + 1\n",
    "                    if (np.max(response) > threshold09):\n",
    "                        predictedOkay09 = predictedOkay09 + 1\n",
    "            totalProcessed = totalProcessed + 1\n",
    "            \n",
    "print(totalProcessed)\n",
    "print(predictedOkay08)\n",
    "print(str((predictedOkay08/totalProcessed)*100)+'%')\n",
    "print(predictedOkay09)\n",
    "print(str((predictedOkay09/totalProcessed)*100)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "i = 0\n",
    "list_of_wrongs = []\n",
    "for index in range(len(xTest)):\n",
    "    res = model.predict(np.expand_dims(xTest[index], axis=0))\n",
    "    predicted = signs_list[np.argmax(res[0])]\n",
    "    expected_result = signs_list[np.argmax(yTest[index])]\n",
    "#     print(predicted)\n",
    "#     print(expected_result)\n",
    "    j += 1\n",
    "    if (predicted == expected_result):\n",
    "        i+=1\n",
    "    else:\n",
    "        list_of_wrongs.append(predicted + expected_result)\n",
    "print(f\"total:{j}, ok: {i}, mal {j - i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4647793e",
   "metadata": {},
   "source": [
    "# GUARDA/CARGA DE MODELO\n",
    "En caso de que el modelo funcione de manera correcta se pueden almacenar los pesos, sino tambien se pueden cargar (descomentar SAVE para guardar, de lo contrario descomentar LOAD para cargar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55c0f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('modelWeightsLSA64_500_1t.h5')\n",
    "model.load_weights('modelos_pesos/GRUx3-GPUmia.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce33525",
   "metadata": {},
   "source": [
    "# PRUEBA EN TIEMPO REAL DEL MODELO\n",
    "\n",
    "Primer celda contiene funciones necesarias para mostrar puntos en imagen, deteccion de persona con MediaPipe (ya definidas y usadas en OBtencion de Datos.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0422d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe\n",
    "\n",
    "mp_holistic = mediapipe.solutions.holistic\n",
    "mp_drawing = mediapipe.solutions.drawing_utils\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)                 #prediction from a frame\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "leftHandStyles = [0,138,255,2,1] #naranja\n",
    "rightHandStyles = [231,217,0,2,1] #celeste\n",
    "faceStyles = [80,110,10,0,1]\n",
    "poseStyles = [70,100,5,2,1]\n",
    "def draw_styled_landmarks(image, results):\n",
    "    #     FACE\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        mp_drawing.DrawingSpec(color=(faceStyles[0],faceStyles[1],faceStyles[2]), thickness=faceStyles[3], circle_radius=faceStyles[4])\n",
    "    )\n",
    "    #     POSE/BODY\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(poseStyles[0],poseStyles[1],poseStyles[2]), thickness=poseStyles[3], circle_radius=poseStyles[4])\n",
    "    )\n",
    "    #     LEFT HAND\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.left_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(leftHandStyles[0],leftHandStyles[1],leftHandStyles[2]), thickness=leftHandStyles[3], circle_radius=leftHandStyles[4])\n",
    "    )\n",
    "    #     RIGHT HAND\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.right_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(rightHandStyles[0],rightHandStyles[1],rightHandStyles[2]), thickness=rightHandStyles[3], circle_radius=rightHandStyles[4])\n",
    "    )\n",
    "\n",
    "def array_from_landmarks(results):\n",
    "    # LEFT HAND array\n",
    "    # len(results.left_hand_landmarks.landmark) = 21 landmarks for each hand, with 2 coordinates each landmark\n",
    "    if (results.left_hand_landmarks):\n",
    "        leftHandLandmarks = np.array([[result.x, result.y] for result in results.left_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        leftHandLandmarks = np.zeros(21*2)\n",
    "\n",
    "\n",
    "    # RIGHT HAND array\n",
    "    # len(results.right_hand_landmarks.landmark) -> same for right hand\n",
    "    if (results.right_hand_landmarks):\n",
    "        rightHandLandmarks = np.array([[result.x, result.y ] for result in results.right_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        rightHandLandmarks = np.zeros(21*2)\n",
    "\n",
    "\n",
    "    # POSE array\n",
    "    # len(results.pose_landmarks.landmark) -> 33 landmarks of 2 coordinates each one (X, Y)\n",
    "    if (results.pose_landmarks):\n",
    "        poseLandmarks = np.array([[result.x, result.y] for result in results.pose_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        poseLandmarks = np.zeros(33*2)\n",
    "\n",
    "\n",
    "    # FACE array\n",
    "    # len(results.face_landmarks.landmark) -> 468 landmarks of 2 coordinates each one\n",
    "    if (results.face_landmarks):\n",
    "        faceLandmarks = np.array([[result.x, result.y] for result in results.face_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        faceLandmarks = np.zeros(468*2)\n",
    "        \n",
    "    return np.concatenate((faceLandmarks, poseLandmarks, leftHandLandmarks, rightHandLandmarks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64e687a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 201, 150) for input KerasTensor(type_spec=TensorSpec(shape=(None, 201, 150), dtype=tf.float32, name='masking_2_input'), name='masking_2_input', description=\"created by layer 'masking_2_input'\"), but it was called on an input with incompatible shape (None, 1, 150).\n"
     ]
    }
   ],
   "source": [
    "sequence = [] #store 40 frames like the framesPerVideo\n",
    "sentence = [] #store a list of 'words'\n",
    "threshold = 0.7\n",
    "\n",
    "\n",
    "caption = cv2.VideoCapture(0)\n",
    "caption.set(cv2.CAP_PROP_FPS, 60)\n",
    "i = 0\n",
    "with mp_holistic.Holistic(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as holisticModel:\n",
    "    while caption.isOpened():\n",
    "        i = i + 1\n",
    "        ret, frame = caption.read()\n",
    "        \n",
    "#         PARA REDIMENSIONAR IMAGEN\n",
    "#         ret, normalFrame = caption.read()\n",
    "#         frame = cv2.resize(normalFrame, (1920, 1080))\n",
    "        image, results = mediapipe_detection(frame, holisticModel)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        keypoints = array_from_landmarks(results)\n",
    "        sequence.append(keypoints[936:]) #le quito datos de la cara\n",
    "#         sequence.append(keypoints)\n",
    "        \n",
    "#         if len(sequence) == maximumNumberOfFrames:\n",
    "        res = model.predict(np.expand_dims(sequence, axis=0))\n",
    "        res = res[0]\n",
    "\n",
    "        if res[np.argmax(res)] > threshold:\n",
    "#           Si predice igual a ultima palabra predicha no se coloca\n",
    "            if len(sentence) > 0:\n",
    "                if signs_list[np.argmax(res)] != sentence[-1]:\n",
    "                    sentence.append(signs_list[np.argmax(res)])\n",
    "            else:\n",
    "                sentence.append(signs_list[np.argmax(res)])\n",
    "#         if len(sentence) > 5:\n",
    "#             sentence = sentence[-5:]\n",
    "\n",
    "        cv2.putText(image, ' '.join(sentence), (10,20), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,255,255), 2)\n",
    "        cv2.putText(image, 'Frame °: ' + str(i), (10,50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,255,255), 2)\n",
    "        cv2.imshow('Predicting sign..', image)\n",
    "    \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'): #press 'q' to break\n",
    "            break\n",
    "    caption.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a75f6",
   "metadata": {},
   "source": [
    "# LIMPIEZA DE LOS DATOS GUARDADOS\n",
    "\n",
    "En esta seccion defino una funcion que salte los frames que representan datos \"limpios\" teniendo en cuenta en este caso como datos limpios a:\n",
    "- Frames que no han detectado como MINIMO 1 mano, cara y posicion. Si falta alguna de estas 3 componentes el frame no se toma en cuenta.\n",
    "\n",
    "- Se debe actualizar el valor de maximumNumberOfFrames segun el caso que se desee probar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a78b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isFrameClean(frame):\n",
    "    if ((frame[0] != 0.0 and frame[1] != 0.0 and frame[934] != 0.0 and frame[935] != 0.0) and \n",
    "        (frame[958] != 0.0 and frame[959] != 0.0 and frame[960] != 0.0 and frame[961] != 0.0) and \n",
    "        ((frame[1018] != 0.0 and frame[1019] != 0.0 and frame[1003] != 0.0 and frame[1004] != 0.0) or \n",
    "        (frame[1044] != 0.0 and frame[1045] != 0.0 and frame[1084] != 0.0 and frame[1085] != 0.0)) \n",
    "        ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "paddingValue = 3\n",
    "\n",
    "def paddData(sequence):\n",
    "    paddLength = maximumNumberOfFrames - len(sequence)\n",
    "    return np.pad(sequence, [(0,paddLength),(0,0)], mode='constant', constant_values=paddingValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA QUE PERMITE EL REEMPLAZO NOMBRADO ARRIBA DEL PROYECTO\n",
    "sequences, labels = [],[]\n",
    "for sign in range(len(signs_list)):\n",
    "    for personNumber in range(numberOfPersons-1):\n",
    "        for videoNumber in range(numberOfVideosPerPerson):\n",
    "            result = np.load(os.path.join(STORE_PATH2, signs_list[sign], f'''0{str(personNumber)}_0{str(videoNumber)}.npy'''))\n",
    "            resultWithFace = []\n",
    "\n",
    "#             el 201 es la longitud de frames con la que se almacenaron todos los videos\n",
    "\n",
    "            for index in range(0,201,2):#me salto de a 1 frame para probar con muestreo de cada video y no usar todos los frames, no es un muestreo probabilistico\n",
    "#                 resultWithoutFace.append(result[index][936:])\n",
    "                resultWithFace.append(result[index])\n",
    "#             al haberme salteado de a 2 frames, de los 201 me quedan 67, \n",
    "#             entonces ahora limpio los frames \"sucios\" y paddeo con valor 3 hasta completar los 67\n",
    "#             despues debería probar que en lugar de completar con valor de padding 3, usar el \n",
    "#             frame previo o siguiente al que se quitó.\n",
    "\n",
    "            cleanedResults = []\n",
    "            for frame in range(len(resultWithFace)):\n",
    "#                 En caso de frame \"sucio\" no debería reemplazarlo por dato de padding, por que el padding debería \n",
    "#                 ir solo al final, por lo que tomo un frame de izquierda o derecha.\n",
    "#                 Sino no reemplazo con nada y hago el padding al final como está ahora\n",
    "                if (isFrameClean(resultWithFace[frame])):\n",
    "                    cleanedResults.append(resultWithFace[frame])\n",
    "            if (len(cleanedResults) != maximumNumberOfFrames):\n",
    "                cleanedResults = paddData(cleanedResults)\n",
    "            sequences.append(cleanedResults)\n",
    "            labels.append(sign_labels[signs_list[sign]])\n",
    "sequences = np.array(sequences)\n",
    "labels = to_categorical(labels).astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
